{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e25ea09-74d6-47e6-bad6-e922f52c3532",
   "metadata": {},
   "source": [
    "# Personal Information\n",
    "Name: **Marissa Faas**\n",
    "\n",
    "StudentID: **14168472**\n",
    "\n",
    "Email: [**14168472l@student.uva.nl**](14168472@student.uva.nl)\n",
    "\n",
    "Submitted on: **19.03.2023**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3cf6243-adfe-4eb8-bba3-bb2835079abd",
   "metadata": {},
   "source": [
    "# Data Context\n",
    "**In this section you should introduce the datasources and datasets which you will be working with. Explain where they are from as well as their domain. Give an overview of what the context of the data is. You should not spend more than 1 to 2 paragraphs here as the core information will be in the next section.**\n",
    "\n",
    "Real world gunshot data, recorded in the wild with wildlife background noise is currenly non-existend. Collecting this audio data would present a significant challenge of setting up microphones in wildlife reserves. This project has not been set-up (yet) since further research is needed to garantee the success of poacher detection in the wild. Therefore this thesis research is of great importance: **To what extent is it possible to achieve sufficient accuracy scores when using a compressed Convolutional Neural Network for gunshot audio detection on an edge device?** However, this entails that the data employed in this thesis will be purposefully generated.\n",
    "\n",
    "The data is collected from the following sources:\n",
    "- Youtube videos for background noise:\n",
    "  - https://www.youtube.com/watch?v=OcVtCTBTJ-4 (\"Nature and wildlife sounds from the African savanna\")\n",
    "  - https://www.youtube.com/watch?v=Bm_Gc4MXqfQ (\"Lions, hyenas and other wildlife calling in the Masai Mara\")\n",
    "  - https://www.youtube.com/watch?v=Mr9T-943BnE (\"Nature Sounds: Rain Sounds One Hour for Sleeping, Sleep Aid for Everybody\")\n",
    "  - https://www.youtube.com/watch?v=T9IJKwEspI8 (\"RELAX OR STUDY WITH NATURE SOUNDS: Ultimate Thunderstorm / 1 hour\")\n",
    "- Publicly available audio dataset UrbanSound8K:\n",
    "  - https://urbansounddataset.weebly.com/urbansound8k.html\n",
    "\n",
    "This results in 4 one hour recordings of savanna by day, savanna by night, rain, and thunder sounds. The UrbanSound8k contains gunshot audio without any background noise, the so called \"signals\", these will be extracted from the dataset. All audio recordings will be combined in different ways to create an augmented dataset for the gunshot audio detection model. All background audio will be cut into snippets of 10 seconds and the gunshot signals will be elongated to 10 seconds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833d964-56e1-49c7-8172-7435357624aa",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "**Present here the results of your exploratory data analysis. Note that there is no need to have a \"story line\" - it is more important that you show your understanding of the data and the methods that you will be using in your experiments (i.e. your methodology).**\n",
    "\n",
    "**As an example, you could show data, label, or group balances, skewness, and basic characterizations of the data. Information about data frequency and distributions as well as results from reduction mechanisms such as PCA could be useful. Furthermore, indicate outliers and how/why you are taking them out of your samples, if you do so.**\n",
    "\n",
    "**The idea is, that you conduct this analysis to a) understand the data better but b) also to verify the shapes of the distributions and whether they meet the assumptions of the methods that you will attempt to use. Finally, make good use of images, diagrams, and tables to showcase what information you have extracted from your data.**\n",
    "\n",
    "As you can see, you are in a jupyter notebook environment here. This means that you should focus little on writing text and more on actually exploring your data. If you need to, you can use the amsmath environment in-line: $e=mc^2$ or also in separate equations such as here:\n",
    "\n",
    "\\begin{equation}\n",
    "    e=mc^2 \\mathrm{\\space where \\space} e,m,c\\in \\mathbb{R}\n",
    "\\end{equation}\n",
    "\n",
    "Furthermore, you can insert images such as your data aggregation diagrams like this:\n",
    "\n",
    "![image](example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534317db-d881-4e33-a358-754e2881e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Imports for signal plot\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582b299-f599-4140-a454-bcbfdeeb273f",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0cf9be-2cac-4227-957f-ad893212e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for Analysis 1: \n",
    "def loadSample(filename):\n",
    "    \"\"\"Input: filename is path to .wav file.\n",
    "    Returns: np.ndarray time series of the audio signal\"\"\"\n",
    "\n",
    "    # Use librosa to load the audio file\n",
    "    s, fs = librosa.load(filename, sr=sample_rate, dtype='float32', mono=True)  \n",
    "  \n",
    "    return s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4df9546-a6d7-4678-aca6-cd13d5f3c79a",
   "metadata": {},
   "source": [
    "### Analysis 1: Signal analysis\n",
    "Plotting time-domain representation of a single audio file to get a feeling of what an audio signal looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a889a6c7-aed8-4a0f-9925-c4f8e2fce1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample rate\n",
    "sample_rate = 48000\n",
    "\n",
    "# Paths\n",
    "datasetPath = \"C:/Users/maris/Documents/DataScience/Thesis/PreviousResearch/Chengeta_model-master/rain/0.wav\"\n",
    "exportPath = \"./plotsMarissa/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b796dc-f69d-4686-b802-bd0d8f679ee8",
   "metadata": {},
   "source": [
    "### Analysis 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33b453f-1bc2-4cad-8021-e548d307f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273bea3-ecaa-4fac-83d0-5fe547b7873d",
   "metadata": {},
   "source": [
    "### Analysis n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60074a1b-1ae5-46e8-971f-100199861c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4bc7a400e35f160b13ed52195005e41b219907c1be09b125a1c17e685484faa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
